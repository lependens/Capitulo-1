# 2.1 Estimaci√≥n de ET‚ÇÄ mediante Redes Neuronales Artificiales (ANN)

Este documento explica paso a paso c√≥mo se implementaron las **Redes Neuronales Artificiales (ANN)** para estimar la evapotranspiraci√≥n de referencia **ET‚ÇÄ**, replicando la metodolog√≠a del TFG original, pero adaptada a **Python/Keras con TensorFlow**. Se integra como continuaci√≥n natural del documento:

**`docs_1.2_Tratamiento de datos y c√°lculo de estimaciones.md`**

---

## üéØ Objetivo

Desarrollar y evaluar **tres modelos de redes neuronales artificiales** que estimen ET‚ÇÄ (mm/d√≠a), usando como referencia los valores calculados con **Penman-Monteith (PM)** en Python.

Estos modelos se comparan con sus equivalentes emp√≠ricos:

| Modelo ANN | Inputs utilizados                                  | Modelo emp√≠rico equivalente |
|------------|----------------------------------------------------|-----------------------------|
| **ANN_Rs** | Radiaci√≥n solar medida (Rs), Temperatura media    | HGR‚Çõ                        |
| **ANN_Ra** | TempMax, TempMin, TempMedia, Radiaci√≥n extraterrestre (Ra) | HGR‚Çê          |
| **ANN_HR** | TempMax, TempMin, TempMedia, Ra, Humedad media    | HGHR                        |

El script `train_nn_et0_fast.py` usa **TensorFlow 2.17.0** (biblioteca de ML de Google) con **Keras** (su API de alto nivel) para construir y entrenar los modelos. TensorFlow maneja el c√≥mputo en GPU/CPU, optimizando el entrenamiento para datasets grandes (~32,500 filas).

---

## üìÅ Estructura del proyecto

```
üìÇ Capitulo-1/
  üìÇ datos_siar_baleares/
     ‚îú‚îÄ IB01_et0_variants.csv  # Datos por estaci√≥n
     ‚îú‚îÄ IB02_et0_variants.csv
     ‚îú‚îÄ ...
     ‚îî‚îÄ IB05_et0_variants.csv
  üìÇ outputs/  # Generados por el script
     ‚îú‚îÄ nn_errors_fast.csv         # Errores por a√±o
     ‚îî‚îÄ nn_errors_summary.csv      # Errores medios resumen
  ‚îú‚îÄ train_nn_et0_fast.py     # Script principal ANN
  ‚îî‚îÄ requirements.txt         # Dependencias
```

- **Ruta base**: `datos_siar_baleares/` (subida manual en Colab o montada desde Drive).
- **Nombre de archivos**: `IBXX_et0_variants.csv` (CSV, UTF-8, separador coma).
- **Formato general**: CSV (pandas.read_csv con encoding='utf-8-sig' para compatibilidad).

---

## üß† Metodolog√≠a aplicada

### ‚úÖ 1. Target (salida de la red)

- Se utiliza **ET0_calc**, calculado con la ecuaci√≥n FAO-56 Penman-Monteith en Python.
- No se usa directamente EtPMon proporcionado por SIAR.
- **Tipo de dato**: Float (mm/d√≠a, normalizado 0-1 con MinMaxScaler durante entrenamiento, denormalizado para m√©tricas).

### ‚úÖ 2. Inputs seg√∫n Tabla 4 del TFG

| Modelo | Inputs utilizados |
|--------|--------------------|
| ANN_Rs | Radiacion, TempMedia |
| ANN_Ra | TempMax, TempMin, TempMedia, Ra |
| ANN_HR | TempMax, TempMin, TempMedia, Ra, HumedadMedia |

- **C√≥mo coge los inputs**: El script carga cada CSV con `pd.read_csv()`, verifica columnas con `if col in df.columns`, y accede con `train_df[inputs].values` (array NumPy float).
- **Formato y tipo de datos**: Float (valores num√©ricos continuos, normalizados 0-1 con MinMaxScaler). El script verifica disponibilidad y avisa si faltan (ej. "Advertencia: Faltan columnas {'Ra'}").

---

### ‚úÖ 3. Arquitectura de la red neuronal

| Par√°metro        | Valor aplicado                  |
|------------------|----------------------------------|
| Capas ocultas    | 1                                |
| Neuronas         | 1 a 10                          |
| Activaci√≥n       | `tanh` (tansig en MATLAB)       |
| Capa de salida   | 1 neurona, activaci√≥n `linear`  |
| Optimizaci√≥n     | Adam (lr=0.001)                |
| P√©rdida          | MSE (Error cuadr√°tico medio)    |
| Early stopping   | S√≠, patience = 1                |
| √âpocas m√°ximas   | 30 (optimizado)                 |
| Batch size       | 128 (optimizado para GPU)       |

- **TensorFlow/Keras**: Usa `Sequential` para capas secuenciales, `Input(shape=(len(inputs),))` para entrada, `Dense` para capas ocultas/salida. Compila con `compile(optimizer='adam', loss='mse')`.
- **Selecci√≥n de modelo**: Por cada combinaci√≥n, selecciona el mejor por MSE en validaci√≥n (generalizaci√≥n) y test (ajuste).
- **Formato de salida del modelo**: Array NumPy float (predicciones denormalizadas, mm/d√≠a).

---

### ‚úÖ 4. Validaci√≥n cruzada por a√±os (K-Fold temporal)

- Cada a√±o del dataset se usa una vez como test.
- El resto de a√±os se dividen en:
  - 85% entrenamiento
  - 15% validaci√≥n
- Se repite para todas las estaciones y todos los modelos ANN.
- **Formato y tipo de datos**: A√±os como int (derivados de 'Fecha' datetime), test/train/val como DataFrames pandas con filas filtradas por a√±o.

---

### ‚úÖ 5. M√©tricas evaluadas

| M√©trica | Descripci√≥n |
|---------|-------------|
| MSE     | Error cuadr√°tico medio |
| RMSE    | Ra√≠z de MSE |
| MAE     | Error absoluto medio |
| R¬≤      | Coeficiente de determinaci√≥n |
| AARE    | Error relativo absoluto medio |

- **Formato y tipo de datos**: Float (precisi√≥n decimal, redondeado a 3 d√≠gitos en resumen).

---

### ‚öôÔ∏è Ejecuci√≥n del script

#### üìå 1. Instalar dependencias

```bash
pip install pandas numpy scikit-learn tensorflow
```

#### üìå 2. Ejecutar el script

```bash
python train_nn_et0_fast.py
```

#### üìå 3. Archivos generados

| Archivo              | Descripci√≥n                           | Formato | Tipo de datos |
|----------------------|---------------------------------------|---------|---------------|
| `nn_errors_fast.csv` | M√©tricas por estaci√≥n, a√±o, modelo    | CSV     | Estacion (str), Modelo (str), Seleccion (str), Test_Year (int), MSE (float), RMSE (float), MAE (float), R2 (float), AARE (float) |
| `nn_errors_summary.csv` | Media de errores por modelo y estaci√≥n | CSV     | Estacion (str), Modelo (str), Seleccion (str), MSE (float), RMSE (float), MAE (float), R2 (float), AARE (float) |

- **Salida del modelo neuronal**: Predicciones de ET‚ÇÄ como array NumPy float (mm/d√≠a, denormalizado de 0-1 a valores reales con inverse_transform).
- **An√°lisis de datos**: Resumen en consola (DataFrame pd.round(3)), con m√©tricas medias por estaci√≥n/modelo/selecci√≥n.

---

### üßæ Fragmento clave del script (`train_nn_et0_fast.py`)

```python
input_combinations = {
    'ANN_Rs': ['Radiacion', 'TempMedia'],
    'ANN_Ra': ['TempMax', 'TempMin', 'TempMedia', 'Ra'],
    'ANN_HR': ['TempMax', 'TempMin', 'TempMedia', 'Ra', 'HumedadMedia']
}

# Ejemplo de entrenamiento
model = Sequential([
    Input(shape=(len(inputs),)),
    Dense(n_neurons, activation='tanh'),
    Dense(1, activation='linear')
])
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')
model.fit(X_train_scaled, y_train_scaled, epochs=30, batch_size=128, verbose=0)
```

---

## ‚úÖ Conclusiones

‚úî ANN_HR (con humedad) presenta mejor desempe√±o.  
‚úî Se replic√≥ la metodolog√≠a del TFG con fidelidad en Python.  
‚úî Los errores medios son comparables o mejores que los modelos emp√≠ricos Hargreaves y Valiantzas.  
‚úî Resultados listos para ser integrados en dashboards o informes.

---

## üöÄ Mejoras futuras

- Implementar Levenberg-Marquardt (como en MATLAB) mediante `tensorflow-probability`.
- Exportar modelos `.h5` para predicci√≥n operativa.
- Aplicar redes LSTM para series temporales. 

Este tutorial es reproducible; ajusta `estaciones` para m√°s datos. ¬°Ejecuta y comp√°rteme `nn_errors_summary.csv` para analizar! üòä